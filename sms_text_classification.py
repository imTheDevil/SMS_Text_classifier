# -*- coding: utf-8 -*-
"""sms_text_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AM94rKDjp17n5mOs4Q0gaUlvYo2GkU5j
"""

# importing libraries, modules
try:
  # %tensorflow_version only exists in Colab.
  !pip install tf-nightly
except Exception:
  pass
import tensorflow as tf
import pandas as pd
from tensorflow import keras
!pip install tensorflow-datasets
import tensorflow_datasets as tfds
import numpy as np
import matplotlib.pyplot as plt

print(tf.__version__)

# getting data files
!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv
!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv

train_file_path = "train-data.tsv"
test_file_path = "valid-data.tsv"

train_dataset = pd.read_csv(train_file_path, sep='\t', header = None)
test_dataset = pd.read_csv(test_file_path, sep='\t', header = None)
train_dataset.tail()

#Replacing 'ham' with 0 and 'spam' with 1 in both test and train datasets
train_dataset[0] = train_dataset[0].replace("ham", 0)
train_dataset[0] = train_dataset[0].replace("spam", 1)
test_dataset[0] = test_dataset[0].replace("ham", 0)
test_dataset[0] = test_dataset[0].replace("spam", 1)

test_dataset.head()

#Makes tensor slices from the dataset
train_data = tf.data.Dataset.from_tensor_slices((train_dataset[1], train_dataset[0]))
test_data = tf.data.Dataset.from_tensor_slices((test_dataset[1], test_dataset[0]))

list(train_data.as_numpy_iterator())[:5]

#Splitting strings to tokens
tokenizer = tfds.deprecated.text.Tokenizer()

#Making a vocabulary set
vocabulary_set = set()
for text_tensor, label in train_data.concatenate(test_data):
  some_tokens = tokenizer.tokenize(text_tensor.numpy())
  vocabulary_set.update(some_tokens)

vocab_size = len(vocabulary_set)
vocab_size

#Creating an encoder object
encoder = tfds.deprecated.text.TokenTextEncoder(vocabulary_set)

def encode(text_tensor, label):
  encoded_text = encoder.encode(text_tensor.numpy())
  return encoded_text, label

def encode_map_fn(text, label):
  encoded_text, label = tf.py_function(encode, 
                                       inp=[text, label], 
                                       Tout=(tf.int64, tf.int64))
  encoded_text.set_shape([None])
  label.set_shape([])

  return encoded_text, label


train_dataset_encoded = train_data.map(encode_map_fn)
test_dataset_encoded = test_data.map(encode_map_fn)

#Example for encoding
for train_example, train_label in train_dataset_encoded.take(2):
  print('Encoded text:', train_example[:10].numpy())
  print('Label:', train_label.numpy())

BUFFER_SIZE = 1000

train_batches = (
    train_dataset_encoded
    .shuffle(BUFFER_SIZE)
    .padded_batch(32))

test_batches = (
    test_dataset_encoded
    .padded_batch(32))

model = keras.Sequential([
  keras.layers.Embedding(encoder.vocab_size, 16),
  keras.layers.GlobalAveragePooling1D(),
  keras.layers.Dense(1, activation='sigmoid')])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.summary()

# train model
history = model.fit(train_batches,
                    epochs=10,
                    validation_data=test_batches,
                    validation_steps=30)

# evaluate model
loss, accuracy = model.evaluate(test_batches)

print("Loss: ", loss)
print("Accuracy: ", accuracy)

# function to predict messages based on model
# returns [x, y] where y being 'ham' or 'spam' and x being their respective probabilities
def predict_message(pred_text):
  encoded_pred_text = encoder.encode(pred_text)
  encoded_pred_text = tf.cast(encoded_pred_text, tf.float32)
  prediction = model.predict(tf.expand_dims(encoded_pred_text, tf.constant(0))).tolist()
  prediction = prediction[0]
  result = []
  if prediction[0] < .5:
    result.append(1-prediction[0])
    result.append("ham")
  else:
    result.append(prediction[0])
    result.append("spam")
  return result

pred_text = "sms WIN to win 1000 now. offer limited time only"

prediction = predict_message(pred_text)
print(prediction)